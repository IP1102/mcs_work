% \noindent
% \section{Introduction}
Refactoring is an important software development activity that employs various techniques to enhance the structure and quality of source code without altering its functionality~\cite{Opdyke1992Refactoring, Fowler1999Refactoring}. 
By removing code smells~\cite{Fowler1999Refactoring} the practice aims to improve maintainability, encapsulating quality attributes such as readability, flexibility, and testability~\cite{ieee1990standard, chawla2015sqmma}.
Refactoring helps maintain high code quality, facilitating long-term maintainability and evolution~\cite{moser2007case}. 

% A major cause for poor code quality is code smells. Code smells are surface-level indicators that often signal deeper, underlying issues within a software system~\cite{Fowler1999Refactoring}. These smells can appear in various forms, such as implementation, design, and architectural smells. While not necessarily problematic on their own, they serve as warning signs of more significant concerns in the codebase. To address these smells and enhance software quality, developers utilize various refactoring techniques. These techniques are tailored to target specific types of smells, aiming to eliminate or mitigate the underlying problems they represent. Singh and Kaur~\cite{singh2018systematic} provide a comprehensive systematic study on this topic, offering valuable insights into different code smells and the most effective refactoring approaches for each.

\Exm{} refactoring is one of the most commonly applied refactoring techniques that involves moving a coherent code fragment from a method into a new, aptly named method~\cite{Fowler1999Refactoring}.
By creating cohesive and smaller methods, extract method refactoring not only improves code quality and maintainability but also serves as a foundation for more complex refactoring operations~\cite{Zarras2015}. 
Extract method refactoring constitutes a significant proportion, approximately $49.6$\%, of the total refactoring recommendations generated by JDeodorant~\cite{JDeodrant}, a widely recognized tool for supporting extract method operations. Furthermore, this refactoring technique has been acknowledged as a crucial operation by both open-source developers~\cite{silva2016we} and industry practitioners~\cite{van2021data}, underscoring its importance in software maintenance.

Automatically performing extract method refactoring, consist of two major steps~\cite{kramer2010legacy}. 
First, \textit{identification} of a candidate method that requires extract method refactoring; 
and second, intelligently \textit{extracting} the logic and \textit{forming} a new method with appropriate parameters, without human intervention. 
% A crucial aspect of software development involves determining whether a source code entity requires refactoring and, if so, identifying the most appropriate refactoring technique for the given context. 
For the first step,
\ie{} identifying a candidate method for the refactoring,
practitioners often rely on intuition and experience.
They also utilize automated tools to assess code quality metrics and detect code smells~\cite{Sharma2018} to get aid in the decision process.
% TS: commented the following because that's not the focus of this study.
% However, these metrics and smell detection mechanisms primarily focus on problem identification, offering limited guidance on the necessity and selection of specific refactoring techniques to address the identified issues or enhance code quality metrics. In recent years, there has been significant progress in automatically identifying refactoring candidates using static metrics and source code properties~\cite{aniche2020effectiveness, karakati2023software} and also intelligent techniques involving machine learning. 
The second step of automated extract method refactoring involves comprehending and extracting source code into a new method. Several approaches have been proposed to address this challenge. Hubert~\cite{hubert2019implementation} developed a method for generating extract method refactoring candidates using static code analysis tools. Maruyama~\cite{hubert2019implementation} proposed a candidate generation technique utilizing block-based slicing. Shahidi \etal{}~\cite{shahidi2022automated} introduced an algorithm for identifying, generating, and ranking extract method candidates through graph analysis. However, these approaches exhibit a few limitations. 
Specifically, most of these approaches require the developers to manually identify the bounds of a block to be refactored \ie{} start and end statements,
to perform the refactoring.
Such a reliance on human knowledge reduces the efficacy and significance of automated refactoring.
Furthermore, static analysis and metric-based methods often fail to capture latent contextual and syntactical code characteristics that could enhance the refactored code.
For instance, these approaches do not offer meaningful identifiers for the new method and its parameters.
% \todo{limitations of the existing approaches is very weak. - The end of next para also has few limitations}

The emergence of \llm{}s (\llmsc{}s)
has enabled the convenience in generative tasks, including code generation with high accuracy ~\cite{wang2021codet5identifierawareunifiedpretrained, ahmad2021}.
% The automated generation of methods following Extract Method refactoring can be conceptualized as a text generation task. Recently, the research community has observed a significant surge in studies employing code \Llm{}s (\llmsc{}s) for text generation. 
The field of code generation has seen significant advancements recently, with pre-trained language models such as GitHub Copilot~\cite{githubGitHubCopilot} and Amazon Q Developer~\cite{amazonCodingAssistant} demonstrating impressive capabilities.
Though such \llmsc{} perform well on many text and code generation tasks, they show mediocre performance for tasks requiring domain-specific or uncommon knowledge. 
For example, \llmsc{} have shown proficiency in generating code for known and common problems but they struggle with unfamiliar problems~\cite{chen2021evaluating}. 
Similarly, in our context,
current \llmsc{} can generate refactored code but often omit the contextual information or generate incomplete, broken, or even uncompilable code~\cite{jha2023codeattack}.
% However, these achievements come with challenges. The process of pre-training language models on extensive code databases is resource-intensive, making it unfeasible for many academic institutions and small companies with limited computational power~\cite{huang2023not}.
Moreover, using third-party code completion services raises privacy concerns for many organizations. A notable example is Samsung Electronics~\cite{JaiVijayan_2023}, which reportedly experienced three data leakage incidents while using online code completion tools such as ChatGPT. These issues highlight the growing need for developing task-specific code generation models. 
% Such models, based on fine-tuning techniques, could be tailored for personal or organizational use, addressing both resource constraints and privacy concerns.

Language models for code are sequence-to-sequence models pre-trained on large corpus of code
and can be fine-tuned for various software engineering tasks, including code summarization~\cite{ahmed2022few, sun2024source}, translation~\cite{eniser2024towards, yin2024rectifier}, completion~\cite{bairi2024codeplan, eghbali2024hallucinator}, bug localization~\cite{yang2024large}, vulnerability detection~\cite{lu2024grace, zhou2024large}, and program repair~\cite{Sharma2024}.
Despite these advancements,
to the best of our knowledge, 
the application of language models for code refactoring remains largely unexplored. 
Inspired by applying \llmsc{}s on a variety of software engineering tasks, there has been some attempts to generate refactored code using them.
For example, a recent contribution by Pomian \etal{}~\cite{Pomian2024} introduced EM-Assist, an IntelliJ IDEA plugin that leverages \llmsc{}s to generate and rank refactoring suggestions using few-shot prompting. 
% \todo{Are they \textbf{generating} code} \todo{- OpenAI API is generating the suggestions but they statically preprocess the suggestions heavily, e.g., using program slicing before applying it.}
% Fine-tuning is another common technique to train a pre-trained model for a specific downstream task.
% However, EM-Assist employs few-shot prompting for the generation of extract method names, which may have limitations. In contrast, fine-tuning a model for a specific task such as \exm{} refactoring potentially allows for more precise adaptation, potentially yielding consistently superior performance compared to few-shot prompting techniques. Furthermore, it is noteworthy that the aforementioned tool solely generates method names, necessitating manual intervention from developers to select the code segments for extraction. This limitation underscores the need for more comprehensive automated solutions in the field of code refactoring. 

Fine-tuning is another common technique to train a pre-trained model for a specific downstream task.
% Recent studies have highlighted significant limitations in existing approaches to automatic code generation, particularly in the context of refactoring. 
While fine-tuning pre-trained code language models appears to be a promising solution, it has been observed that a considerable portion of programs generated by these models often fail to pass unit tests~\cite{chen2021evaluating, li2022competition, jha2023codeattack}.
Such challenges deter the adoption of the automated refactoring tools and methods.
% This is particularly problematic for extract method refactoring, where maintaining the functionality of the method or class is paramount.

To address these challenges, we evaluate performance of fine-tuned models and
propose a deep reinforcement learning approach that aligns fine-tuned code language models to generate refactored code by applying extract method refactoring automatically. 
% Our approach builds upon recent advancements in reinforcement learning for code generation and completion, such as the work by Wang \etal{}~\cite{wang2022compilable} on generating compilable code with compiler feedback, CodeRL by Lee \etal{}~\cite{le2022coderl}, PPOCoder by \etal{}~\cite{shojaee2023execution}, and IRCoCo by Li \etal{}~\cite{li2024ircoco}. 
% Our framework encompasses several key steps. 
% We begin by utilizing the SEART tool to collect repositories and employ RefactoringMiner to identify methods for refactoring. 
Our approach, first, creates a dataset using state-of-the-art tools such as RefactoringMiner~\cite{Tsantalis:ICSE:2018:RefactoringMiner, Tsantalis:TSE:2020:RefactoringMiner2.0}.
We use the dataset to fine-tune four language models, pre-trained on code, using Supervised Fine Tuning (\sft{})~\cite{howard2018universal, kenton2019bert}
% \todo{cite}. 
% Initial evaluation using metrics such as BLEU and CodeBLEU showed promising results. However, qualitative analysis revealed that only $33$\% of the generated methods passed their corresponding unit tests, highlighting the limitations of the SFT approach for code generation tasks.
To enhance model performance and better align it with the objective of generating compilable code while preserving functionality, we use Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} for reinforcement learning optimization.

Our reinforcement learning approach utilizes an actor-critic architecture~\cite{konda1999actor, wan2018improving}, where the actor component generates refactored code, and the critic component assesses the quality of the generated code. This architecture enables the model to learn more efficiently in the complex space of code refactoring by providing guidance on the desirability of different refactoring decisions.
The critic component incorporates discrete, non-differentiable reward signals in three stages. We first check for syntactic correctness, then assess whether the code compiles successfully, and finally, we use RefactoringMiner to detect if the desired refactoring has been applied.

To strike a balance between generating refactorings and maintaining the knowledge gained during supervised fine-tuning, we introduce a Kullback-Leibler (KL) divergence~\cite{kullback1997information, shojaee2023execution} term in the reward function. This term measures the difference between the model's current behavior and its initial behavior learned during supervised fine-tuning. By incorporating this term, we encourage the model to explore new refactoring strategies while preventing it from deviating too far from its initial understanding of code refactoring. 

Our study yielded promising results. 
The \plbart{} model,
when fine-tuned using supervised learning,
demonstrates superior performance among the chosen models
when evaluated using conventional metrics such as \bleu{},
\rouge{}, and \codebleu{}.
However, \codetf{} outperforms other models when trained with deep reinforcement learning.
\textbf{We observe that combining supervised fine-tuning with deep reinforcement learning prove most effective},
compared to fine-tuning the models or training using reinforcement learning individually.
Qualitative evaluation further validates that the combination works the best, exhibiting enhanced syntactic accuracy, compilation rates, and unit test performance. 

We list the key contributions of this thesis below.
\begin{itemize}
    \item We evaluate the effectiveness of supervised fine-tuned models for automatic \exm{} refactoring.
    The approach addresses the limitations of existing approaches such as manual code selection to specify the code block to-be extracted.
    \item The study presents a hybrid method that combines supervised fine-tuning with reinforcement learning optimization, specifically tailored for extract method refactoring tasks. We then evaluate the approach both quantitatively and qualitatively to ensure that it generates syntactically and semantically accurate refactorings.
    \item This study also contributes a tool for analyzing Java repositories on GitHub to create an extract method refactoring datasets with associated metadata. We provide the tool and the dataset created using it for replication and extension purposes.
\end{itemize}
