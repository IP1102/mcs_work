
% \todo{Add a section on usage of RL in SE - DONE}

\subsection{Automated Refactoring}

Many studies have explored automated refactoring candidate identification using machine learning techniques. Typically, these studies use source code metrics or commit messages to train models. Aniche \etal{}~\cite{Aniche2020Effectiveness} predict $20$ kinds of refactorings at method, class, or variable levels using code, process, and ownership metrics, with Random Forest performing best among six algorithms. Gerling~\cite{Gerling2020Machine} extended this work by improving the data collection process to create a high-quality, large-scale refactoring dataset. Van Der Leij \etal{}~\cite{van2021data} analyze five machine learning models to predict Extract Method refactoring, comparing results with industry experts. Using $61$ code metrics, they also found Random Forest to be the best performing model. Kumar \etal{}~\cite{Kumar2019Method} studied method-level refactoring prediction, analyzing 10 machine learning classifiers. Sagar \etal{}~\cite{Sagar2021Comparing} approaches refactoring candidate prediction as a multi-class classification problem, using both source code quality metrics and commit messages as features to predict six method-level refactorings. They compare text-based and source code-based models. Kurbatova \etal{}~\cite{Kurbatova2020Recommendation} employ code embeddings generated from Code2Vec~\cite{Alon2019Code2vec} to train their model for Move Method refactoring prediction.

In the domain of automated code refactoring, researchers have developed a variety of specialized tools and approaches. CeDAR~\cite{tairas2012cedar}, an Eclipse plugin, focuses on identifying and eliminating duplicate code. JDeodorant~\cite{JDeodrant, mazinanian2016jdeodorant} detects code smells and proposes refactoring strategies. Fokaefs~\etal{}\cite{fokaefs2012identification} extended JDeodorant's capabilities to prioritize and implement class extraction refactorings. SOMOMOTO\cite{zanetti2014automated} facilitates move method refactoring and code modularization. While these rule-based methods have made significant contributions, they face limitations in capturing semantic information during refactoring. Moreover, they often require manual intervention from developers to identify and select code blocks for refactoring. To address these challenges, recent research has explored the application of deep learning and large language models (LLMs) for automated code refactoring.

Szalontai~\etal{}~\cite{szalontai2023deep} developed a deep learning method for refactoring source code, initially designed for the Erlang programming language. Their approach comprises a localizer and a refactoring component, enabling the identification and transformation of non-idiomatic code patterns into idiomatic counterparts. Tufano~\etal{}~\cite{tufano2019learning} conducted a quantitative investigation into the potential of Neural Machine Translation (NMT) models for automatically applying code changes implemented during pull requests. Their approach leverages NMT to translate code components from their pre-pull request state to their post-pull request state, effectively simulating developer-implemented changes. To facilitate the rename refactoring process and reduce cognitive load on developers, Liu~\etal{}~\cite{liu2023refbert} proposed RefBERT, a two-stage pre-trained framework based on the BERT architecture. RefBERT is designed to automatically suggest meaningful variable names, addressing a challenging aspect of code refactoring.


Current automated refactoring tools lack semantic understanding and require manual intervention. To address this, we propose a hybrid approach combining supervised fine-tuning with reinforcement learning, enhancing the accuracy and completeness of extract method refactoring. This is the first study to apply deep reinforcement learning for this task, advancing automated refactoring tools.

\subsection{Reinforcement learning in software engineering}

Sequence modeling has emerged as a fundamental paradigm for addressing a wide array of software engineering challenges. In recent years, researchers have explored the application of deep reinforcement learning (DRL) techniques to mitigate exposure bias in supervised fine-tuned models for sequence generation tasks~\cite{ranzato2016sequenceleveltrainingrecurrent, keneshloo2019deepreinforcementlearningsequence}. Notably, Ranzato~\etal{}~\cite{ranzato2016sequenceleveltrainingrecurrent} pioneered the use of established metrics such as BLEU and ROUGE as reward signals in DRL algorithms to optimize network parameters in machine translation, effectively addressing exposure bias. The intersection of DRL and sequence modeling has led to innovative frameworks, such as the one proposed by Chen~\etal{}~\cite{chen2021decisiontransformerreinforcementlearning}, which reconceptualizes reinforcement learning problems as sequence modeling tasks. This approach has paved the way for novel applications in various domains.

In the realm of software engineering, DRL methods have gained traction, particularly in code completion and summarization tasks. Wang~\etal{}\cite{wang2022compilable} leveraged compiler feedback as a reward signal to enhance the quality of language model-generated code. Le\etal{}\cite{le2022coderl} introduced CodeRL, a framework that integrates RL with unit test signals to fine-tune program synthesis models. Shojaee~\etal{}~\cite{shojaee2023execution} conducted comprehensive research, proposing a framework for fine-tuning code language models using DRL and execution signals as rewards. Recent advancements in this field include IRCOCO by Li~\etal{}\cite{li2024ircoco}, which employs immediate rewards to fine-tune language models for code completion tasks. Wang\etal{}\cite{wang2024rlcoder} developed RLCoder, combining DRL with Retrieval-Augmented Generation (RAG) pipelines for repository-level code completion. Furthermore, Nichols\etal{}~\cite{nichols2024performance} demonstrated the potential of DRL in generating efficient parallel code, expanding the application of these techniques to performance optimization.

To our knowledge, \llm{}s have not been specifically trained or aligned for extract method refactoring. Our approach, which combines supervised fine-tuning with PPO alignment, is a first in this domain. This novel methodology produces accurate refactored methods, marking a significant advancement in the field.